# Parse-and-Pipe + CDN Architecture

## âœ… **Core Feasibility**

The parse-and-pipe approach with CDN caching is **fully feasible** with your existing stack. Here's why:

### **Proven Pattern:**
The stream.tee() method branches the readablestream into two: one for the browser and one for Supabase Storage, with EdgeRuntime.waitUntil uploading the audio stream to storage in the background while immediately returning the streaming response to the client.

### **Gemini TTS Compatibility:**
Gemini TTS in Vertex AI API supports unidirectional streaming where the client sends a single request and receives a stream of responses, outputting PCM 16bit 24kHz audio data

### **CDN Caching:**
Supabase Storage uses a Smart CDN that caches assets globally, with cache status sent in the cf-cache-status header where HIT indicates the object was served directly from CDN

With Smart CDN enabled (Pro plan+), assets are cached at the edge as long as possible, with automatic cache invalidation when files are updated or deleted

---

## ğŸ“Š **Updated Timeline: Parse-and-Pipe + CDN**

### **First Session (New Analysis):**
```
Time 0s:      Video analysis starts (Gemini 2.0 Flash streaming)
Time 3-5s:    First feedback `message` field completes in JSON stream
Time 3.1s:    SSML generation starts for first message (Gemini LLM)
Time 4.1s:    TTS streaming request starts (Gemini 2.5 Flash-TTS)
Time 4.2s:    First audio chunk arrives (75-135ms TTFB)
              â”œâ”€ Branch 1: Stream to client immediately
              â””â”€ Branch 2: Upload to Supabase Storage (background)
Time 4.2s:    Client starts buffering audio chunks
Time 5.2s:    Video playback starts
Time 10.5s:   User hears first word (after buffering ~5s of audio)
Time 10.5-23.5s: First feedback plays (13 seconds)
Time 10.5-30s: During playback:
              - Feedback 2-4 generate sequentially (SSML â†’ TTS â†’ stream)
              - All uploads to storage complete (background)
              - CDN cache populated for all segments
Time 30s:     Analysis complete, all audio cached
```

### **Subsequent Sessions (CDN Cache Hit):**
```
Time 0s:      Request for same feedback audio
Time 0.1s:    CDN cache HIT - audio served instantly âš¡
              No TTS generation needed
```

**Performance Gain:**
- First session: **20-50s â†’ 4-5s** (80-90% faster)
- Repeat session: **20-50s â†’ 0.1s** (99.8% faster) âš¡âš¡

---

## ğŸ—ï¸ **Architecture Components**

### **1. Streaming JSON Parser**
- **Library:** `@streamparser/json-whatwg` (Deno/Supabase Edge Function compatible)
- **Purpose:** Extract first feedback object as it arrives (~3-5s into LLM stream)
- **Configuration:**
  ```typescript
  const parser = new JSONParser({
    paths: ['$.feedback[*]'],  // Emit complete feedback objects
    emitPartialValues: false   // Wait for complete objects
  });
  ```

### **2. SSML Generation (Existing)**
- **Timing:** ~1s after message completion
- **Input:** Feedback message + voice config
- **Output:** SSML text for TTS

### **3. Gemini TTS Streaming**
- **Model:** `gemini-2.5-flash-tts`
- **Output:** PCM 16-bit 24kHz audio stream
- **Latency:** 75-135ms time-to-first-byte
- **Chunk Size:** Fixed 1920-byte packets equal to exactly 40ms of LINEAR16 PCM audio (960 samples Ã— 2 bytes at 24kHz)

### **4. Stream Branching (ElevenLabs Pattern)**
```typescript
// Edge Function response handler
const ttsStream = await geminiTTS.generateStream(ssml);

// Branch: one to client, one to storage
const [clientStream, storageStream] = ttsStream.tee();

// Return immediately to client
const response = new Response(clientStream, {
  headers: {
    'Content-Type': 'audio/pcm',
    'Transfer-Encoding': 'chunked'
  }
});

// Upload in background (non-blocking)
EdgeRuntime.waitUntil(
  uploadAudioToStorage(storageStream, audioPath)
);

return response;
```

### **5. Storage + CDN Caching**
- **Path pattern:** `processed/audio/analysis_${analysisId}/segment_${index}_${hash}.pcm`
- **Hash includes:** message + voice config + timestamp (for cache invalidation)
- **CDN behavior:** Automatic edge caching with `cf-cache-status` header
- **Cache check:** Query storage first before generating new audio

---

## ğŸ¯ **Integration with Current Pipeline**

### **Current 2-Phase Architecture:**
```
Phase 1 (INSERT trigger â†’ /webhook):
â”œâ”€ Video analysis (Gemini 2.5 Video)
â”œâ”€ Feedback generation (text only)
â””â”€ Status: analysis_complete

Phase 2 (UPDATE trigger â†’ /post-analyze):
â”œâ”€ SSML generation (all feedback)
â”œâ”€ TTS generation (all feedback, sequential)
â””â”€ Status: completed
```

### **New Parse-and-Pipe Architecture:**
```
Phase 1 (INSERT trigger â†’ /webhook):
â”œâ”€ Video analysis streaming (Gemini 2.0 Flash)
â”œâ”€ Parse first feedback from JSON stream (3-5s)
â”œâ”€ Generate SSML for first feedback
â”œâ”€ Stream TTS audio to client + storage
â”‚  â”œâ”€ Client: immediate playback
â”‚  â””â”€ Storage: background upload â†’ CDN cache
â”œâ”€ Continue processing feedback 2-N sequentially
â”‚  â””â”€ Each: SSML â†’ TTS stream â†’ storage
â””â”€ Status: completed (all audio cached)

Phase 2: ELIMINATED (or becomes cache validation/retry)
```

**Key Architectural Change:**
- Single-phase processing with progressive audio generation
- First feedback ready in 4-5s (vs 20-50s)
- No UPDATE trigger needed (everything in Phase 1)

---

## ğŸ”‘ **Critical Implementation Details**

### **Storage Path Design (for CDN caching):**
```typescript
// Deterministic hash for cache hits
const contentHash = createHash('sha256')
  .update(feedback.message)
  .update(voiceConfig.voice_name)
  .update(voiceConfig.prompt_personality)
  .digest('hex')
  .substring(0, 16);

const audioPath = `processed/audio/analysis_${analysisId}/segment_${index}_${contentHash}.pcm`;

// Cache check before generating
const { data: cachedAudio } = await supabase.storage
  .from('processed')
  .download(audioPath);

if (cachedAudio) {
  // CDN hit - return cached audio instantly
  return new Response(cachedAudio, {
    headers: {
      'Content-Type': 'audio/pcm',
      'cf-cache-status': 'HIT'  // CDN served this
    }
  });
}

// Cache miss - generate new audio
const ttsStream = await generateGeminiTTS(ssml);
// ... proceed with stream.tee() pattern
```

### **Client-Side Playback (React Native):**
The client uses Web Audio API (AudioContext) to schedule and play chunks as they arrive rather than waiting to construct a valid Blob URL at the end - this is an advanced low-latency approach to streaming speech in real time

**Requirements:**
- Use `react-native-audio-api` or `@fugood/react-native-audio-pcm-stream`
- Queue PCM chunks as they stream in
- Start playback after ~2-3s buffer (not waiting for complete file)
- Handle 24kHz 16-bit PCM format

### **Database Schema Changes:**
```sql
-- No changes needed to analysis_audio_segments
-- Just update the workflow:

analysis_audio_segments (
  analysis_id,
  segment_index,
  timestamp_seconds,
  audio_path,  -- Now includes content hash for caching
  message,
  created_at
)

-- Add optional cache metadata
ALTER TABLE analysis_audio_segments 
ADD COLUMN content_hash text,
ADD COLUMN cache_status text;  -- 'HIT' | 'MISS' | 'GENERATED'
```

---

## ğŸ“ˆ **Performance Expectations**

### **Latency Breakdown (First Session):**
| Step | Time | Cumulative |
|------|------|------------|
| Video analysis starts | 0s | 0s |
| First feedback parsed | 3-5s | 3-5s |
| SSML generation | 1s | 4-6s |
| TTS TTFB | 0.1s | 4.1-6.1s |
| Client buffering | 1s | **5.2-7.2s** âœ… |
| **User hears audio** | - | **~5-7s** |

### **Improvement vs Current:**
- Current: 20-50s wait time
- Parse-and-pipe: 5-7s wait time
- **Improvement: 75-90% faster** âš¡

### **Repeat Session (Cache Hit):**
- Storage query + CDN delivery: **~100ms**
- **Improvement: 99.5% faster than current** âš¡âš¡

---

## âš ï¸ **Implementation Considerations**

### **1. Streaming Complexity**
- Streaming JSON parsing requires careful error handling
- Partial parse failures must gracefully fall back
- Need correlation IDs for debugging stream issues

### **2. Storage Upload Limitations**
- Supabase Storage doesn't support true streaming uploads
- Must collect chunks in memory, then upload complete file
- Adds ~500ms-1s overhead (but happens in background via `waitUntil`)

### **3. Client Buffer Management**
- React Native needs native module for PCM streaming
- Requires development build (not Expo Go)
- Must handle network interruptions gracefully

### **4. Cache Invalidation**
- Voice config changes invalidate all cached audio
- Content hash must include all TTS-affecting parameters
- Need strategy for purging old cached segments

### **5. Error Recovery**
- If streaming fails mid-generation, need fallback to batch mode
- Database must track which segments are cached vs need regeneration
- Client should handle partial audio delivery

---

## ğŸ’¡ **Recommended Implementation Strategy**

### **Phase 1: Core Streaming Pipeline**
1. Implement streaming JSON parser for first feedback
2. Add SSML â†’ TTS streaming for first segment
3. Stream audio directly to client (no storage yet)
4. Validate 5-7s time-to-first-audio

### **Phase 2: Add CDN Caching**
1. Implement `stream.tee()` branching
2. Add background storage upload with `EdgeRuntime.waitUntil`
3. Implement cache-check-before-generate logic
4. Add content hash to audio paths

### **Phase 3: Sequential Processing**
1. After first audio streams, process feedback 2-N
2. Each: parse â†’ SSML â†’ TTS â†’ stream to storage
3. Update progress incrementally (20% â†’ 40% â†’ 60% â†’ 100%)

### **Phase 4: Client Optimizations**
1. Implement PCM chunk queueing in React Native
2. Add smart buffering (start playback at 2-3s)
3. Handle cache hits (instant playback)

---

## ğŸ¯ **Expected User Experience**

### **First Video Analysis:**
1. User uploads video (existing flow)
2. **After 5-7 seconds:** "Analyzing..." â†’ First feedback starts playing âœ…
3. Video playback syncs with audio
4. While user watches/listens, remaining feedback generates in background
5. Entire analysis feels fast and responsive

### **Subsequent Analyses:**
1. User uploads new video
2. System detects similar feedback (content hash matches)
3. **Instant audio playback from CDN** (0.1s) âš¡
4. Only unique feedback needs generation
5. User experiences near-instantaneous coaching

---

This architecture delivers your primary goal: **drastically reducing user waiting time** from 20-50s to 5-7s, with cached playback at ~0.1s for repeated patterns. The sequential generation of feedback 2-N happens during the 13-second playback window of feedback 1, making it completely transparent to users.